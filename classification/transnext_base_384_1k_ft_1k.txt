$ python main.py --config configs/finetune/transnext_base_384_ft.py --data-path data/imagenet/ --resume checkpoints/transnext_base_384_1k_ft_1k.pth --eval
swattention package not found, loading PyTorch native version of Aggregated Attention
Not using distributed mode
Namespace(fp32_resume=False, batch_size=64, epochs=5, update_freq=1, config='configs/finetune/transnext_base_384_ft.py', model='transnext_base', input_size=384, pretrain_size=224, fixed_pool_size=None, drop=0.0, drop_path=0.8, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=1.0, momentum=0.9, weight_decay=0.05, sched=None, lr=1e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='data/imagenet/', data_set='IMNET', use_mcloader=False, inat_category='name', output_dir='checkpoints/transnext_base_384', device='cuda', seed=0, resume='checkpoints/transnext_base_384_1k_ft_1k.pth', start_epoch=0, eval=True, dist_eval=False, num_workers=10, pin_mem=True, compile_model=True, cache_size_limit=64, world_size=1, dist_url='env://', distributed=False)
Warping 384 size input images...
Creating model: transnext_base
C:\Python311\Lib\site-packages\torch\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3484.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
number of params: 89721496
LR = 0.00001000
Batch size = 64
Update frequent = 1
<All keys matched successfully>
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
Test:  [  0/521]  eta: 3:48:41  loss: 0.1795 (0.1795)  acc1: 97.9167 (97.9167)  acc5: 98.9583 (98.9583)  time: 26.3372  data: 20.9986  max mem: 13656
Test:  [ 10/521]  eta: 0:32:26  loss: 0.1795 (0.2785)  acc1: 96.8750 (93.9394)  acc5: 100.0000 (99.0530)  time: 3.8099  data: 1.9090  max mem: 13657
Test:  [ 20/521]  eta: 0:22:50  loss: 0.3565 (0.4130)  acc1: 92.7083 (90.3770)  acc5: 98.9583 (99.0575)  time: 1.5557  data: 0.0000  max mem: 13657
Test:  [ 30/521]  eta: 0:19:19  loss: 0.5346 (0.4886)  acc1: 87.5000 (88.7769)  acc5: 98.9583 (98.7231)  time: 1.5637  data: 0.0001  max mem: 13657
Test:  [ 40/521]  eta: 0:17:15  loss: 0.5346 (0.5648)  acc1: 82.2917 (86.4329)  acc5: 97.9167 (98.2724)  time: 1.5385  data: 0.0002  max mem: 13657
Test:  [ 50/521]  eta: 0:15:53  loss: 0.3332 (0.5004)  acc1: 92.7083 (88.2557)  acc5: 98.9583 (98.4681)  time: 1.5042  data: 0.0003  max mem: 13657
Test:  [ 60/521]  eta: 0:14:59  loss: 0.2451 (0.4753)  acc1: 93.7500 (89.0027)  acc5: 98.9583 (98.5485)  time: 1.5408  data: 0.0002  max mem: 13657
Test:  [ 70/521]  eta: 0:14:12  loss: 0.3599 (0.4653)  acc1: 92.7083 (89.3779)  acc5: 98.9583 (98.5622)  time: 1.5492  data: 0.0003  max mem: 13657
Test:  [ 80/521]  eta: 0:13:32  loss: 0.2382 (0.4408)  acc1: 91.6667 (89.9949)  acc5: 98.9583 (98.6497)  time: 1.5127  data: 0.0002  max mem: 13657
Test:  [ 90/521]  eta: 0:12:58  loss: 0.3308 (0.4641)  acc1: 90.6250 (89.4574)  acc5: 98.9583 (98.5234)  time: 1.5041  data: 0.0001  max mem: 13657
Test:  [100/521]  eta: 0:12:27  loss: 0.5280 (0.4752)  acc1: 88.5417 (89.2533)  acc5: 97.9167 (98.4220)  time: 1.5047  data: 0.0001  max mem: 13657
Test:  [110/521]  eta: 0:11:59  loss: 0.4276 (0.4686)  acc1: 90.6250 (89.3769)  acc5: 97.9167 (98.4328)  time: 1.5047  data: 0.0001  max mem: 13657
Test:  [120/521]  eta: 0:11:34  loss: 0.3878 (0.4717)  acc1: 90.6250 (89.3251)  acc5: 97.9167 (98.3643)  time: 1.5041  data: 0.0003  max mem: 13657
Test:  [130/521]  eta: 0:11:10  loss: 0.4885 (0.4800)  acc1: 87.5000 (88.9393)  acc5: 97.9167 (98.3858)  time: 1.5039  data: 0.0003  max mem: 13657
Test:  [140/521]  eta: 0:10:47  loss: 0.4139 (0.4685)  acc1: 91.6667 (89.1770)  acc5: 98.9583 (98.4264)  time: 1.5040  data: 0.0001  max mem: 13657
Test:  [150/521]  eta: 0:10:25  loss: 0.3987 (0.4928)  acc1: 91.6667 (88.6176)  acc5: 98.9583 (98.3927)  time: 1.5038  data: 0.0002  max mem: 13657
Test:  [160/521]  eta: 0:10:04  loss: 0.6013 (0.4905)  acc1: 86.4583 (88.7358)  acc5: 98.9583 (98.4213)  time: 1.5039  data: 0.0002  max mem: 13657
Test:  [170/521]  eta: 0:09:44  loss: 0.3589 (0.4848)  acc1: 91.6667 (88.8950)  acc5: 98.9583 (98.4223)  time: 1.5047  data: 0.0002  max mem: 13657
Test:  [180/521]  eta: 0:09:24  loss: 0.3589 (0.4797)  acc1: 90.6250 (89.0251)  acc5: 98.9583 (98.4576)  time: 1.5075  data: 0.0002  max mem: 13657
Test:  [190/521]  eta: 0:09:05  loss: 0.4700 (0.4808)  acc1: 90.6250 (88.9234)  acc5: 98.9583 (98.4893)  time: 1.5075  data: 0.0003  max mem: 13657
Test:  [200/521]  eta: 0:08:46  loss: 0.6065 (0.4925)  acc1: 86.4583 (88.6868)  acc5: 97.9167 (98.4142)  time: 1.5081  data: 0.0004  max mem: 13657
Test:  [210/521]  eta: 0:08:28  loss: 0.5294 (0.4902)  acc1: 86.4583 (88.6996)  acc5: 97.9167 (98.4400)  time: 1.5073  data: 0.0004  max mem: 13657
Test:  [220/521]  eta: 0:08:10  loss: 0.5294 (0.5031)  acc1: 86.4583 (88.3437)  acc5: 98.9583 (98.3786)  time: 1.5042  data: 0.0003  max mem: 13657
Test:  [230/521]  eta: 0:07:52  loss: 0.5438 (0.5058)  acc1: 85.4167 (88.2080)  acc5: 97.9167 (98.3541)  time: 1.5042  data: 0.0001  max mem: 13657
Test:  [240/521]  eta: 0:07:34  loss: 0.4704 (0.5122)  acc1: 85.4167 (87.9582)  acc5: 97.9167 (98.3014)  time: 1.5049  data: 0.0002  max mem: 13657
Test:  [250/521]  eta: 0:07:17  loss: 0.5758 (0.5226)  acc1: 84.3750 (87.7490)  acc5: 96.8750 (98.2238)  time: 1.5055  data: 0.0002  max mem: 13657
Test:  [260/521]  eta: 0:07:00  loss: 0.6683 (0.5383)  acc1: 82.2917 (87.2845)  acc5: 95.8333 (98.1561)  time: 1.5058  data: 0.0003  max mem: 13657
Test:  [270/521]  eta: 0:06:43  loss: 0.8319 (0.5487)  acc1: 76.0417 (86.9619)  acc5: 95.8333 (98.0896)  time: 1.5067  data: 0.0001  max mem: 13657
Test:  [280/521]  eta: 0:06:26  loss: 0.8015 (0.5575)  acc1: 77.0833 (86.7549)  acc5: 96.8750 (98.0501)  time: 1.5063  data: 0.0000  max mem: 13657
Test:  [290/521]  eta: 0:06:09  loss: 0.5945 (0.5587)  acc1: 84.3750 (86.7268)  acc5: 97.9167 (98.0205)  time: 1.5149  data: 0.0000  max mem: 13657
Test:  [300/521]  eta: 0:05:53  loss: 0.3890 (0.5551)  acc1: 89.5833 (86.8252)  acc5: 97.9167 (98.0066)  time: 1.5251  data: 0.0001  max mem: 13657
Test:  [310/521]  eta: 0:05:36  loss: 0.5221 (0.5612)  acc1: 88.5417 (86.6794)  acc5: 97.9167 (97.9736)  time: 1.5161  data: 0.0002  max mem: 13657
Test:  [320/521]  eta: 0:05:19  loss: 0.5870 (0.5598)  acc1: 84.3750 (86.7179)  acc5: 97.9167 (97.9589)  time: 1.5058  data: 0.0002  max mem: 13657
Test:  [330/521]  eta: 0:05:03  loss: 0.5870 (0.5719)  acc1: 85.4167 (86.4017)  acc5: 97.9167 (97.8443)  time: 1.5060  data: 0.0002  max mem: 13657
Test:  [340/521]  eta: 0:04:47  loss: 0.6040 (0.5728)  acc1: 82.2917 (86.3056)  acc5: 97.9167 (97.8403)  time: 1.5169  data: 0.0002  max mem: 13657
Test:  [350/521]  eta: 0:04:31  loss: 0.5796 (0.5785)  acc1: 82.2917 (86.1408)  acc5: 97.9167 (97.8187)  time: 1.5361  data: 0.0003  max mem: 13657
Test:  [360/521]  eta: 0:04:15  loss: 0.7212 (0.5812)  acc1: 81.2500 (86.0919)  acc5: 96.8750 (97.7868)  time: 1.5467  data: 0.0003  max mem: 13657
Test:  [370/521]  eta: 0:03:59  loss: 0.6270 (0.5832)  acc1: 84.3750 (86.0147)  acc5: 97.9167 (97.7903)  time: 1.5345  data: 0.0001  max mem: 13657
Test:  [380/521]  eta: 0:03:43  loss: 0.5617 (0.5820)  acc1: 86.4583 (86.0701)  acc5: 97.9167 (97.7854)  time: 1.5198  data: 0.0001  max mem: 13657
Test:  [390/521]  eta: 0:03:27  loss: 0.5710 (0.5852)  acc1: 85.4167 (85.9655)  acc5: 97.9167 (97.7701)  time: 1.5309  data: 0.0003  max mem: 13657
Test:  [400/521]  eta: 0:03:11  loss: 0.6705 (0.5862)  acc1: 84.3750 (85.9856)  acc5: 97.9167 (97.7504)  time: 1.5384  data: 0.0003  max mem: 13657
Test:  [410/521]  eta: 0:02:55  loss: 0.7229 (0.5891)  acc1: 85.4167 (85.9337)  acc5: 96.8750 (97.7038)  time: 1.5386  data: 0.0002  max mem: 13657
Test:  [420/521]  eta: 0:02:39  loss: 0.5677 (0.5877)  acc1: 87.5000 (85.9981)  acc5: 96.8750 (97.6989)  time: 1.5478  data: 0.0001  max mem: 13657
Test:  [430/521]  eta: 0:02:23  loss: 0.5677 (0.5954)  acc1: 86.4583 (85.7913)  acc5: 96.8750 (97.6508)  time: 1.5545  data: 0.0001  max mem: 13657
Test:  [440/521]  eta: 0:02:07  loss: 0.7641 (0.6031)  acc1: 80.2083 (85.5915)  acc5: 95.8333 (97.5931)  time: 1.5592  data: 0.0001  max mem: 13657
Test:  [450/521]  eta: 0:01:51  loss: 0.7347 (0.6054)  acc1: 81.2500 (85.5183)  acc5: 96.8750 (97.5795)  time: 1.5580  data: 0.0001  max mem: 13657
Test:  [460/521]  eta: 0:01:36  loss: 0.6127 (0.6056)  acc1: 84.3750 (85.4257)  acc5: 97.9167 (97.5868)  time: 1.5601  data: 0.0002  max mem: 13657
Test:  [470/521]  eta: 0:01:20  loss: 0.6081 (0.6085)  acc1: 85.4167 (85.3525)  acc5: 97.9167 (97.5562)  time: 1.5522  data: 0.0004  max mem: 13657
Test:  [480/521]  eta: 0:01:04  loss: 0.6898 (0.6139)  acc1: 84.3750 (85.1958)  acc5: 97.9167 (97.5528)  time: 1.5431  data: 0.0002  max mem: 13657
Test:  [490/521]  eta: 0:00:48  loss: 0.4608 (0.6117)  acc1: 84.3750 (85.2279)  acc5: 98.9583 (97.5624)  time: 1.5592  data: 0.0002  max mem: 13657
Test:  [500/521]  eta: 0:00:33  loss: 0.4228 (0.6118)  acc1: 88.5417 (85.2025)  acc5: 97.9167 (97.5694)  time: 1.5782  data: 0.0003  max mem: 13657
Test:  [510/521]  eta: 0:00:17  loss: 0.6707 (0.6175)  acc1: 79.1667 (85.0518)  acc5: 97.9167 (97.5518)  time: 1.5561  data: 0.0002  max mem: 13657
Test:  [520/521]  eta: 0:00:01  loss: 0.5570 (0.6149)  acc1: 79.1667 (85.1080)  acc5: 97.9167 (97.5580)  time: 1.6422  data: 0.0002  max mem: 13657
Test: Total time: 0:13:42 (1.5788 s / it)
* Acc@1 85.108 Acc@5 97.558 loss 0.615
Accuracy of the network on the 50000 test images: 85.1%