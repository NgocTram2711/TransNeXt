$ python main.py --config configs/transnext_tiny.py --data-path data/imagenet/ --resume checkpoints/transnext_tiny_224_1k.pth --eval
swattention package not found, loading PyTorch native version of Aggregated Attention
Not using distributed mode
Namespace(fp32_resume=False, batch_size=128, epochs=300, update_freq=1, config='configs/transnext_tiny.py', model='transnext_tiny', input_size=224, pretrain_size=None, fixed_pool_size=None, drop=0.0, drop_path=0.25, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=1.0, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='data/imagenet/', data_set='IMNET', use_mcloader=False, inat_category='name', output_dir='checkpoints/transnext_tiny', device='cuda', seed=0, resume='checkpoints/transnext_tiny_224_1k.pth', start_epoch=0, eval=True, dist_eval=False, num_workers=10, pin_mem=True, compile_model=True, cache_size_limit=128, world_size=1, dist_url='env://', distributed=False)
Creating model: transnext_tiny
C:\Python311\Lib\site-packages\torch\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\aten\src\ATen\native\TensorShape.cpp:3484.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
number of params: 28258960
LR = 0.00100000
Batch size = 128
Update frequent = 1
<All keys matched successfully>
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
swattention package not found, loading PyTorch native version of Aggregated Attention
Test:  [  0/261]  eta: 1:45:57  loss: 0.3894 (0.3894)  acc1: 92.7083 (92.7083)  acc5: 98.4375 (98.4375)  time: 24.3593  data: 21.5689  max mem: 6271
Test:  [ 10/261]  eta: 0:11:51  loss: 0.4026 (0.4869)  acc1: 92.7083 (89.1572)  acc5: 98.9583 (98.9583)  time: 2.8358  data: 1.9610  max mem: 6271
Test:  [ 20/261]  eta: 0:07:13  loss: 0.7309 (0.6421)  acc1: 81.7708 (84.5982)  acc5: 97.9167 (97.7927)  time: 0.6708  data: 0.0002  max mem: 6271
Test:  [ 30/261]  eta: 0:05:30  loss: 0.5377 (0.5661)  acc1: 89.5833 (87.3152)  acc5: 97.9167 (97.9671)  time: 0.6583  data: 0.0001  max mem: 6271
Test:  [ 40/261]  eta: 0:04:34  loss: 0.3723 (0.5293)  acc1: 92.1875 (88.4782)  acc5: 98.4375 (98.0945)  time: 0.6584  data: 0.0001  max mem: 6271
Test:  [ 50/261]  eta: 0:03:57  loss: 0.5298 (0.5642)  acc1: 88.5417 (87.6226)  acc5: 97.3958 (97.8248)  time: 0.6582  data: 0.0001  max mem: 6271
Test:  [ 60/261]  eta: 0:03:31  loss: 0.5617 (0.5642)  acc1: 86.9792 (87.7135)  acc5: 97.3958 (97.8654)  time: 0.6610  data: 0.0001  max mem: 6271
Test:  [ 70/261]  eta: 0:03:10  loss: 0.5564 (0.5663)  acc1: 86.9792 (87.4853)  acc5: 98.4375 (97.9607)  time: 0.6622  data: 0.0001  max mem: 6271
Test:  [ 80/261]  eta: 0:02:53  loss: 0.5474 (0.5765)  acc1: 87.5000 (87.2685)  acc5: 98.9583 (97.9874)  time: 0.6660  data: 0.0001  max mem: 6271
Test:  [ 90/261]  eta: 0:02:38  loss: 0.5460 (0.5691)  acc1: 87.5000 (87.3970)  acc5: 98.9583 (98.0082)  time: 0.6754  data: 0.0002  max mem: 6271
Test:  [100/261]  eta: 0:02:25  loss: 0.5915 (0.5810)  acc1: 86.4583 (87.1132)  acc5: 98.4375 (97.9631)  time: 0.6753  data: 0.0003  max mem: 6271
Test:  [110/261]  eta: 0:02:12  loss: 0.6852 (0.5964)  acc1: 83.8542 (86.6929)  acc5: 97.9167 (97.8463)  time: 0.6701  data: 0.0002  max mem: 6271
Test:  [120/261]  eta: 0:02:01  loss: 0.6950 (0.6136)  acc1: 81.7708 (86.2345)  acc5: 96.3542 (97.6928)  time: 0.6764  data: 0.0002  max mem: 6271
Test:  [130/261]  eta: 0:01:51  loss: 0.8143 (0.6461)  acc1: 78.6458 (85.4286)  acc5: 95.3125 (97.4356)  time: 0.6844  data: 0.0003  max mem: 6271
Test:  [140/261]  eta: 0:01:41  loss: 0.9016 (0.6652)  acc1: 75.5208 (84.9291)  acc5: 95.3125 (97.2702)  time: 0.6727  data: 0.0002  max mem: 6271
Test:  [150/261]  eta: 0:01:31  loss: 0.8196 (0.6686)  acc1: 80.2083 (84.9200)  acc5: 95.8333 (97.1923)  time: 0.6634  data: 0.0002  max mem: 6271
Test:  [160/261]  eta: 0:01:22  loss: 0.6926 (0.6748)  acc1: 84.8958 (84.8182)  acc5: 95.3125 (97.0659)  time: 0.6687  data: 0.0003  max mem: 6271
Test:  [170/261]  eta: 0:01:13  loss: 0.8925 (0.6940)  acc1: 76.5625 (84.2806)  acc5: 93.7500 (96.8811)  time: 0.6693  data: 0.0003  max mem: 6271
Test:  [180/261]  eta: 0:01:04  loss: 0.9362 (0.7027)  acc1: 76.0417 (84.0211)  acc5: 94.2708 (96.7887)  time: 0.6629  data: 0.0005  max mem: 6271
Test:  [190/261]  eta: 0:00:56  loss: 0.8110 (0.7057)  acc1: 82.2917 (83.9796)  acc5: 95.3125 (96.7550)  time: 0.6584  data: 0.0005  max mem: 6271
Test:  [200/261]  eta: 0:00:47  loss: 0.7498 (0.7152)  acc1: 81.2500 (83.7324)  acc5: 95.8333 (96.6729)  time: 0.6581  data: 0.0003  max mem: 6271
Test:  [210/261]  eta: 0:00:39  loss: 0.8017 (0.7228)  acc1: 81.2500 (83.5900)  acc5: 94.7917 (96.5418)  time: 0.6686  data: 0.0004  max mem: 6271
Test:  [220/261]  eta: 0:00:31  loss: 0.9271 (0.7359)  acc1: 78.6458 (83.2626)  acc5: 93.7500 (96.4131)  time: 0.6833  data: 0.0003  max mem: 6271
Test:  [230/261]  eta: 0:00:23  loss: 0.9298 (0.7398)  acc1: 77.0833 (83.1056)  acc5: 94.7917 (96.3948)  time: 0.6855  data: 0.0002  max mem: 6271
Test:  [240/261]  eta: 0:00:16  loss: 0.7793 (0.7481)  acc1: 80.7292 (82.8449)  acc5: 96.3542 (96.3498)  time: 0.6856  data: 0.0002  max mem: 6271
Test:  [250/261]  eta: 0:00:08  loss: 0.7072 (0.7449)  acc1: 81.7708 (82.9163)  acc5: 96.3542 (96.3666)  time: 0.6875  data: 0.0001  max mem: 6271
Test:  [260/261]  eta: 0:00:00  loss: 0.6100 (0.7474)  acc1: 82.8125 (82.8700)  acc5: 97.3958 (96.3700)  time: 0.7151  data: 0.0000  max mem: 6271
Test: Total time: 0:03:19 (0.7662 s / it)
* Acc@1 82.870 Acc@5 96.370 loss 0.747
Accuracy of the network on the 50000 test images: 82.9%